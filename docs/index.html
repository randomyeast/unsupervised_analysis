
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Welcome to the documentation for the Kennedy lab’s unsupervised analysis tools! &#8212; Kennedy lab unsupervised analysis tools 0.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/classic.css" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css" />
    
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="vq-triplet-treba.lib" href="lib.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="lib.html" title="vq-triplet-treba.lib"
             accesskey="N">next</a> |</li>
        <li class="nav-item nav-item-0"><a href="#">Kennedy lab unsupervised analysis tools 0.0 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Welcome to the documentation for the Kennedy lab’s unsupervised analysis tools!</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="welcome-to-the-documentation-for-the-kennedy-lab-s-unsupervised-analysis-tools">
<h1>Welcome to the documentation for the Kennedy lab’s unsupervised analysis tools!<a class="headerlink" href="#welcome-to-the-documentation-for-the-kennedy-lab-s-unsupervised-analysis-tools" title="Permalink to this heading">¶</a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>These pages contain documentation relevant for unsupervised
analysis methods used in Ann Kennedy’s lab at Northwestern
University.</p></li>
<li><p>There are currently three models available for use: the
Trajectory Variational Autoencoder (TVAE), TREBA, and Vector
Quantized TREBA with triplet loss (VQ-TREBA).</p></li>
<li><p>To generate clusters corresponding to behavioral motifs,
the TVAE and TREBA both require some post-hoc clustering
of the generated embeddings and VQ-TREBA does not.</p></li>
<li><p>The goal of this documentation is both to explain how each
model variant works and make it as easy as possible to adapt
the models to work on new datasets.</p></li>
<li><p>The documentation for each model is split up into a reference
document, meant for those implementing / modifying the models,
as well as a section with more explicit mathematical details.</p></li>
<li><p>Any suggestions are more than welcome.</p></li>
</ul>
</section>
</section>
<section id="quickstart-guide">
<h1>Quickstart guide<a class="headerlink" href="#quickstart-guide" title="Permalink to this heading">¶</a></h1>
<section id="setting-up-a-new-experiment">
<h2>Setting up a new experiment<a class="headerlink" href="#setting-up-a-new-experiment" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>To setup the directory structure for a new experiment, in the terminal
type <code class="docutils literal notranslate"><span class="pre">bash</span> <span class="pre">setup.bash</span> <span class="pre">&lt;your_new_project_name&gt;</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">./experiments/&lt;your_new_project_name&gt;/checkpoints</span></code> will be where the
model checkpoints will be stored.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">./experiments/&lt;your_new_project_name/log</span></code> will be where the log used
to visualize the loss in tensorboard will be stored.</p></li>
</ul>
</li>
<li><p>Next, you should create a copy of the template configuration file available in
<code class="docutils literal notranslate"><span class="pre">./experiments/example_tvae/config.json</span></code> and add the copy to
<code class="docutils literal notranslate"><span class="pre">./experiments/&lt;your_new_project_name/</span></code>. The <code class="docutils literal notranslate"><span class="pre">config.json</span></code> has three dictionaries
within it:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">data_config</span></code>: This is passed to the instantiation of a new dataset object.
Dataset objects will be explained in the third step. Any additional parameters
needed for loading and preprocessing data should be added here. After you create
a new dataset object in the next step, you will need to change the <code class="docutils literal notranslate"><span class="pre">name</span></code>
parameter here to reflect the new dataset object you created as well as in
<code class="docutils literal notranslate"><span class="pre">./lib/util/datasets/__init__.py</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_config</span></code>: This is used to pick the model architecture being used (e.g.
<code class="docutils literal notranslate"><span class="pre">TVAE</span></code> in the example configuration) and to configure the dimensions of the
different layers in the model. If you’re creating a new dataset object, the
main parameter you should update here is <code class="docutils literal notranslate"><span class="pre">state_dim</span></code> which represents the
dimensionality of each frame of each trajectory in the inputs. For additional
information on what the different keys represent, check the model documentation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train_config</span></code>: This is used to configure training parameters such as the
batch size, device to use for training, how many epochs etc.</p>
<ul>
<li><p>One important thing to recognize is that <code class="docutils literal notranslate"><span class="pre">num_epochs</span></code> is a list.
This is because some models use staged training. The current support
for and explanation of different stages of training is in the
documentation for each model.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="structuring-the-folders-containing-your-data">
<h2>Structuring the folders containing your data<a class="headerlink" href="#structuring-the-folders-containing-your-data" title="Permalink to this heading">¶</a></h2>
<ul>
<li><p>In all likelihood, you are learning representations of video data. Much of the
code in this repo assumes your data has the following structure:</p>
<img alt="_images/data_structure.png" id="file-structure" src="_images/data_structure.png" />
<p>where <code class="docutils literal notranslate"><span class="pre">vid_1</span></code>, <code class="docutils literal notranslate"><span class="pre">vid_2</span></code>, and <code class="docutils literal notranslate"><span class="pre">vid_3</span></code> corresponds to directories which
contain the data you are learning representations of. The outputs of the
model corresponding to a given video will be stored in that video’s directory.</p>
</li>
</ul>
</section>
<section id="creating-a-new-dataset-object">
<h2>Creating a new dataset object<a class="headerlink" href="#creating-a-new-dataset-object" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>To train any of the model variants, you first need to create
a new dataset object which inherits from the base module
<a class="reference internal" href="datasets.html#lib.util.datasets.core.TrajectoryDataset" title="lib.util.datasets.core.TrajectoryDataset"><code class="xref py py-mod docutils literal notranslate"><span class="pre">TrajectoryDataset</span></code></a>.
An example dataset is available here:
<code class="xref py py-mod docutils literal notranslate"><span class="pre">MouseV1Dataset</span></code></p></li>
<li><p>The first method from <a class="reference internal" href="datasets.html#lib.util.datasets.core.TrajectoryDataset" title="lib.util.datasets.core.TrajectoryDataset"><code class="xref py py-mod docutils literal notranslate"><span class="pre">TrajectoryDataset</span></code></a>
you need to override in your new dataset object is <code class="docutils literal notranslate"><span class="pre">load_data(data_config)</span></code>.</p>
<ul>
<li><p>This function is called to load the trajectories you are interested in
embedding and should return a <code class="docutils literal notranslate"><span class="pre">np.array</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code> of the
shape <code class="docutils literal notranslate"><span class="pre">[num_trajs,</span> <span class="pre">traj_len,</span> <span class="pre">num_features]</span></code></p></li>
<li><p>There should be a <code class="docutils literal notranslate"><span class="pre">root_data_dir</span></code> field in your <code class="docutils literal notranslate"><span class="pre">data_config</span></code> that
is the path to the video directories explained in the previous step.</p></li>
</ul>
</li>
<li><p>The second method you will need to override is the static method <code class="docutils literal notranslate"><span class="pre">load_video</span></code>
which tells <a class="reference internal" href="datasets.html#lib.util.datasets.core.TrajectoryDataset" title="lib.util.datasets.core.TrajectoryDataset"><code class="xref py py-mod docutils literal notranslate"><span class="pre">TrajectoryDataset</span></code></a>
how to load a single video’s data for your given dataset. The output expected
for <code class="docutils literal notranslate"><span class="pre">load_video</span></code> is <code class="docutils literal notranslate"><span class="pre">[num_trajs,</span> <span class="pre">traj_len,</span> <span class="pre">num_features]</span></code></p></li>
<li><p>The third method you need to override is <code class="docutils literal notranslate"><span class="pre">preprocess(data_config,</span> <span class="pre">trajectories)</span></code>.
This is where you will do any preprocessing of the data loaded in via <code class="docutils literal notranslate"><span class="pre">load_data</span></code>.
This function must also return a <code class="docutils literal notranslate"><span class="pre">np.array</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code> of the shape
<code class="docutils literal notranslate"><span class="pre">[num_trajs,</span> <span class="pre">traj_len,</span> <span class="pre">num_features]</span></code>.</p></li>
<li><p>The fourth method you need to override is <code class="docutils literal notranslate"><span class="pre">postprocess(trajectories)</span></code>. If you
perform any preprocessing of your input data (e.g. decomposition) and you want
to see what the reconstructions of the original inputs look like, you should
make this function perform the inverse of whatever your postprocessing steps
are.</p></li>
</ul>
</section>
<section id="training-the-model">
<h2>Training the model<a class="headerlink" href="#training-the-model" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>Running <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">train.py</span> <span class="pre">--config_dir</span> <span class="pre">./experiments/&lt;your_experiment_name&gt;</span></code> will
train the model. It will likely take several hours to complete, depending on your
hardware and how much data you are using.</p></li>
<li><p>Once the model has completed training, there will be a directory titled <code class="docutils literal notranslate"><span class="pre">stage_&lt;x&gt;</span></code>
within the <code class="docutils literal notranslate"><span class="pre">./experiments/&lt;your_experiment_name&gt;/</span></code> directory, for each stage of
training you have. Within each of these directories, there will be a file called
<code class="docutils literal notranslate"><span class="pre">best.pt</span></code> which will be the checkpoint with the lowest negative-log-likelihood
on the evaluation set for that stage.</p></li>
<li><p>To monitor the model’s training progress with tensorboard, in the terminal type:
<code class="docutils literal notranslate"><span class="pre">tensorboard</span> <span class="pre">--logdir=./experiments/example_tvae</span></code> and click on link to a port
on <code class="docutils literal notranslate"><span class="pre">localhost</span></code> that is printed to the terminal. To monitor the training process
across machines see this note &lt;link&gt;.</p></li>
</ul>
</section>
<section id="generating-reconstructions">
<h2>Generating reconstructions<a class="headerlink" href="#generating-reconstructions" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>Running <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">reconstruct.py</span> <span class="pre">--config_dir</span> <span class="pre">./experiments/&lt;your_experiment_name&gt;</span></code> will
generate reconstructions for all videos in the <code class="docutils literal notranslate"><span class="pre">root_data_dir</span></code> specified
in <code class="docutils literal notranslate"><span class="pre">eval_config</span></code> and outputs them to their respective folders.</p></li>
<li><p>You can also generate random reconstructions from one of the videos in <code class="docutils literal notranslate"><span class="pre">root_data_dir</span></code>
using <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">plot_example_reconstructions.py</span> <span class="pre">--config_dir</span>
<span class="pre">./experiments/&lt;your_experiment_name&gt;</span> <span class="pre">--num_reconstructions</span> <span class="pre">&lt;some_integer&gt;</span></code>. The output
of this will be stored in a <code class="docutils literal notranslate"><span class="pre">random_reconstructions</span></code> folder within your experiment
folder.</p></li>
<li><p>All supported models are based on the variational autoencoder and the reconstruction
process samples <code class="docutils literal notranslate"><span class="pre">eval_config[num_samples]</span></code> embeddings from the inferred posterior,
decodes each embedding to get a reconstruction, and then picks the “best” reconstruction
as the one with the lowest negative-log-likelihood. Increasing <code class="docutils literal notranslate"><span class="pre">eval_config[num_samples]</span></code>
will dramatically increase the amount of time it takes for <code class="docutils literal notranslate"><span class="pre">reconstruct.py</span></code> to complete,
but you may get more realistic reconstructions of the input data.</p></li>
</ul>
</section>
<section id="generating-embeddings-using-the-tvae">
<h2>Generating embeddings using the TVAE<a class="headerlink" href="#generating-embeddings-using-the-tvae" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>Running <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">embed.py</span> <span class="pre">--config_dir</span> <span class="pre">./experiments/&lt;your_experiment_name&gt;</span></code> will
iterate through and generate embeddings for all of the video subdirectories in the
<code class="docutils literal notranslate"><span class="pre">root_data_dir</span></code> specified in <code class="docutils literal notranslate"><span class="pre">eval_config</span></code></p></li>
<li><p>All of the models currently supported are based on the variational autoencoder
architecture and use the mean of the inferred posterior for each input as the
embedding for each trajectory.</p></li>
</ul>
</section>
<section id="treba-quickstart-guide">
<h2>TREBA quickstart guide<a class="headerlink" href="#treba-quickstart-guide" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>Under construction</p></li>
</ul>
</section>
<section id="vq-treba-quickstart-guide">
<h2>VQ-TREBA quickstart guide<a class="headerlink" href="#vq-treba-quickstart-guide" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>Under construction</p></li>
</ul>
</section>
<section id="module-documentation">
<h2>Module documentation<a class="headerlink" href="#module-documentation" title="Permalink to this heading">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="lib.html">vq-triplet-treba.lib</a><ul>
<li class="toctree-l2"><a class="reference internal" href="distributions.html">lib.distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="models.html">lib.models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="tvae.html">lib.models.tvae</a><ul>
<li class="toctree-l4"><a class="reference internal" href="tvae_encoder_explanation.html">TVAE encoder explanation</a></li>
<li class="toctree-l4"><a class="reference internal" href="tvae_decoder_explanation.html">TVAE decoder explanation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="util.html">lib.util</a><ul>
<li class="toctree-l3"><a class="reference internal" href="datasets.html">lib.util.datasets</a><ul>
<li class="toctree-l4"><a class="reference internal" href="mouse_v1.html">lib.util.datasets.mouse_v1</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</section>
</section>
<section id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this heading">¶</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="#">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Welcome to the documentation for the Kennedy lab’s unsupervised analysis tools!</a><ul>
<li><a class="reference internal" href="#overview">Overview</a></li>
</ul>
</li>
<li><a class="reference internal" href="#quickstart-guide">Quickstart guide</a><ul>
<li><a class="reference internal" href="#setting-up-a-new-experiment">Setting up a new experiment</a></li>
<li><a class="reference internal" href="#structuring-the-folders-containing-your-data">Structuring the folders containing your data</a></li>
<li><a class="reference internal" href="#creating-a-new-dataset-object">Creating a new dataset object</a></li>
<li><a class="reference internal" href="#training-the-model">Training the model</a></li>
<li><a class="reference internal" href="#generating-reconstructions">Generating reconstructions</a></li>
<li><a class="reference internal" href="#generating-embeddings-using-the-tvae">Generating embeddings using the TVAE</a></li>
<li><a class="reference internal" href="#treba-quickstart-guide">TREBA quickstart guide</a></li>
<li><a class="reference internal" href="#vq-treba-quickstart-guide">VQ-TREBA quickstart guide</a></li>
<li><a class="reference internal" href="#module-documentation">Module documentation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#indices-and-tables">Indices and tables</a></li>
</ul>

  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="lib.html"
                          title="next chapter">vq-triplet-treba.lib</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/index.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="lib.html" title="vq-triplet-treba.lib"
             >next</a> |</li>
        <li class="nav-item nav-item-0"><a href="#">Kennedy lab unsupervised analysis tools 0.0 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Welcome to the documentation for the Kennedy lab’s unsupervised analysis tools!</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2022, Andrew Ulmer (adapted from Jennifer J. Sun).
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.0.2.
    </div>
  </body>
</html>